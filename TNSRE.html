<!DOCTYPE html>
<html>    
<head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>SensingAIG1</title>

  
  <link rel="stylesheet" href="https://unmannedlab.github.io//assets/css/bootstrap/bootstrap.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">
  
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/research.css">
  <link rel="canonical" href="https://unmannedlab.github.io//research/RELLIS-3D">
  <link rel="alternate" type="application/atom+xml" title="Unmanned Systems Lab" href="https://unmannedlab.github.io//feed.xml" />

  <!-- Icons -->
  <link rel="unmanned_favicon" sizes="32x32" href="https://unmannedlab.github.io//assets/icons/favicon.ico"> 
  <link rel="shortcut icon" type="image/x-icon" href="siteicon.ico">
  <link rel="stylesheet" href="/assets/icons/font-awesome/css/font-awesome.min.css">
  
</head>


<body>
<style>
    .title a {
        text-decoration: none;
    }

    .topnav-right {
        float: right;
        font-size: 24px; 
        vertical-align:middle; 
        line-height: 64px;
        display: none;
    }
	body{
		margin: 10px 225px;
	}

    #myLinks{
        display: none;
    }
    @media only screen and (max-width: 600px) {
        .topnav-right {
            display: inline-block;
        }

        .nav-large{
            display:none;
        }
    }
</style>

<script src="/js/menu.js"></script> 

<header class="header" ">
    <div class="header-container">
        <div class="title">
            <a href="/"> <img src="images/logo_03.jpg" width="300" height=86> </a> 

            <div class="topnav-right">
                <a href="javascript:void(0);" class="icon" onclick="myFunction()"> <i class="fa fa-bars"></i> </a>
            </div>
            <div id="myLinks">
                <br>
            </div>
        </div> 
    </div>
</header>
    <br>
    <div class="page-content">
      <div class="wrapper">
        <div class="post">
  <br>
  <header class="post-header">
    <h1 class="display-4" align="center">Dynamic Crosswalk Scene Understanding for the Visually Impaired</h1>
    <p class="post-meta" align="center">By: Guoguang Hua</p>
  </header>

  <article class="post-content">
    <p align="center">
	<a href="https://www.tamu.edu/"><img src="images/logo_03.jpg" alt="Shenzhen University" height="80" width="300" /></a>
	<a href="https://www.insa-rennes.fr/"><img src="images/INSA.png" alt="INSA Rennes" height="80" width="300" /></a><br />
	<a href="https://www.arl.army.mil/"><img src="images/guangdong.jpg" alt="CCDC Army Research Laboratory" height="86" width="654" /></a></p>

<p align="center">
Shishun Tian<sup>1,3</sup>, Minghuo Zheng<sup>1,3</sup>, Wenbin Zou<sup>1, 3,*</sup>, Xia Li<sup>1,3</sup>, and Lu Zhang<sup>2</sup><br />
1. <a href="http://ceie.szu.edu.cn/">Shenzhen University; </a>&emsp;2. <a href="https://www.insa-rennes.fr/">INSA-Rennes; </a> &emsp;3. <a href="http://iip.szu.edu.cn/">Guangdong KLIIP;<br />
<a href="">[Website]</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9481094">[Paper]</a> <a href="https://github.com/SensingAI/WVD-SZU/">[Github]</a>
</p>

<h2 id="overview">Overview</h2>
<p>Independent mobility poses a great challenge to the visually impaired individuals. This paper proposes a novel system to understand dynamic crosswalk scenes,
which detects the key objects, such as crosswalk, vehicle, and pedestrian, and identifies pedestrian traffic light status. The indication of where and when to cross the road is provided to the visually impaired based on the crosswalk scene
understanding. Our proposed system is implemented on a head-mounted mobile device (SensingAI G1) equipped with an Intel RealSense camera and a cellphone, and provides
surrounding scene information to visually impaired individuals through audio signal. To validate the performance of the proposed system, we propose a crosswalk scene
understanding dataset which contains three sub-datasets: a pedestrian traffic light dataset with 7447 images, a dataset of key objects on the crossroad with 1006 images and a
crosswalk dataset with 3336 images. Extensive experiments demonstrated that the proposedsystem was robust and outperformed the state-of-the-art approaches. The experiment
conducted with the visually impaired subjects shows that the system is practical useful. </p>

<p align="center"><img src="images/com.jpg" width="850"/></p> 

<h2 id="annotated-data" align="center">Method Overview</h2>
<h3 id="ontology">CcossWalk Detector</h3>
<p>A crosswalk is composed of several bright white stripes and dark background, so parallel straight edges of crosswalk stripes can be recognized as a kind of feature of crosswalks. The
bright stripes alternate with a dark background in crosswalks, so the periodic gray value distribution is also a feature of crosswalks. In this paper, a novel crosswalk detection algorithm is
proposed based on these features, as shown in following figure. </p>
		 	 
<p>Candidate extraction and analysis are two components that constitute the detector. The former extracts candidates based on the feature of bright white stripes, the latter
analysis candidates using the feature of parallel straight edges.</p>
<p align="center"><img src="images/CrosswalkDetector.png" width="655" class="center" /></p>
										    
<h3 id="images-statics">Pedestrian Traffic Light Recognition</h3>
<p>The pedestrian lights are used at intersections to notify the pedestrians when to cross the street. The pedestrian traffic light detection provides the visually impaired with the status
of pedestrian signals. In this paper, candidates for PTL are detected by a YOLOv4 based key objects detector adapted for localizing and recognizing PTL. The object detector provides
the boundaries (x, y, w, h) and objectness confidence score that quantifies the classification confidence and location of an object.</p>
		       
<p>The candidates, including the PTL, VTL and the extinguished traffic light, are detected by the key object detector. However, only PTL is needed for blind navigation in this
paper. Pedestrian traffic light recognition is proposed to prune the unqualified ones, as shown in following figure. The same as the proposed crosswalk detector, first, the candidate (figure(b)) is
extracted from RGB image (figure(a)) by the object dector. Then it is converted to gray image (figure(c)) and HSV image (figure(d)) to obtain the texture and color information. Next,
a traffic light mask (figure(e)), containing both red and green regions, is extracted from HSV image. Then the shape of traffic light (figure(f)) is generated by multiplying the traffic light
mask to gray image. The minimum bounding rectangle (MBR) of the shape of traffic light is highlighted with yellow dotted lines in figure(f).</p>
		       
<p align="center"><img src="images/trafficrecognition.png" width="455" class="center" /></p>


<h2 id="benchmarks" align="center">Experiment Results and Analysis</h2>

<h3 id="image-semantic-segmenation">CrossWalk Detection</h3>
				   <h4 id="image-semantic-segmenation">The qualitative comparison of crosswalk detection algorithms in
different scenarios</h4>
<p align="center"><img src="images/qualitative.png" width="485" class="center" /></p>
 			           <h4 id="image-semantic-segmenation">The results of ablation study</h4>
<p align="center"><img src="images/ablation.jpg" width="600" class="center" /></p>
									   
<h3 id="lidar-semantic-segmenation">Pedestrian Traffic Light Recognition</h3>
				   <h4 id="image-semantic-segmenation">The improvements of our methods</h4>
<p align="center"><img src="images/improvement.jpg" width="700" class="center" /></p>
					<h4 id="image-semantic-segmenation">The detected results by YOLOv4 and our methods</h4>
<p align="center"><img src="images/detected.png" width="600" class="center" /></p>


<h2 id="data-download">Data Download</h2>
	<p>Baidu Cloud Disk:</p>
		     <ul>
			<li>Link:<a href="https://pan.baidu.com/s/13FX4TSVszbgo-T3aoEW1Xg?pwd=n609">https://pan.baidu.com/s/13FX4TSVszbgo-T3aoEW1Xg?pwd=n609</a></li>
		     </ul>
	<p>Google Cloud Disk:</p>
		     <ul>
			<li>Link:<a href="https://drive.google.com/drive/folders/1cMQwkv2UlqRPX-lnQT9dxmv9eiLgLViw?usp=drive_link">https://drive.google.com/drive/folders/1cMQwkv2UlqRPX-lnQT9dxmv9eiLgLViw?usp=drive_link</a></li>
		     </ul>
											  

									 
									 
<h2 id="citation">Citation</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@ARTICLE{9481094,
  author={Tian, Shishun and Zheng, Minghuo and Zou, Wenbin and Li, Xia and Zhang, Lu},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Dynamic Crosswalk Scene Understanding for the Visually Impaired}, 
  year={2021},
  volume={29},
  number={},
  pages={1478-1486},
  doi={10.1109/TNSRE.2021.3096379}}
</code></pre></div></div>

<h2 id="collaborator">Collaborator</h2>
<p><a href="https://www.arl.army.mil/"><img src="images/SensingAi.png" alt="Research Laboratory" width="358" height="182" class="center" /></a></p>

<h2 id="license">License</h2>
<p>All datasets and code on this page are copyright by us and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License.</p>

  </article>

	<div class="ds-thread" data-thread-key=/research/RELLIS-3D data-title=RELLIS-3D: A Multi-modal Dataset for Off-Road Robotics data-url=//research/RELLIS-3D></div>


</div>

      </div>
     
    </div>
  </body>
  <footer class="footer">
    <div class="footer-container">
        <div class="container">
            <div class="section left">
                &#169; 2023 SensingAI <br>
                CoLL. of Electronics and Information Engineering <br>
                Shenzhen University
            </div>
            <div class="section centerd">
                <br>
                <style>
    footer a:visited{
        color: #404040;
    }

    footer a:hover{
        color: #303030;
    }
</style>
            </div>
        
        </div>
    </div>
</footer>

<script src="https://unmannedlab.github.io//js/jquery.slim.min.js"></script>
<script src="https://unmannedlab.github.io//js/bootstrap.min.js"></script>

</html>
