<!DOCTYPE html>
<html>    
<head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>SensingAIG1</title>

  
  <link rel="stylesheet" href="https://unmannedlab.github.io//assets/css/bootstrap/bootstrap.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">
  
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/research.css">
  <link rel="canonical" href="https://unmannedlab.github.io//research/RELLIS-3D">
  <link rel="alternate" type="application/atom+xml" title="Unmanned Systems Lab" href="https://unmannedlab.github.io//feed.xml" />

  <!-- Icons -->
  <link rel="unmanned_favicon" sizes="32x32" href="https://unmannedlab.github.io//assets/icons/favicon.ico"> 
  <link rel="shortcut icon" href="https://unmannedlab.github.io//assets/icons/favicon.ico">
  <link rel="stylesheet" href="/assets/icons/font-awesome/css/font-awesome.min.css">
  
</head>


<body>
<style>
    .title a {
        text-decoration: none;
    }

    .topnav-right {
        float: right;
        font-size: 24px; 
        vertical-align:middle; 
        line-height: 64px;
        display: none;
    }
	body{
		margin: 10px 225px;
	}

    #myLinks{
        display: none;
    }

    @media only screen and (max-width: 600px) {
        .topnav-right {
            display: inline-block;
        }

        .nav-large{
            display:none;
        }
    }
</style>

<script src="/js/menu.js"></script> 

<header class="header" ">
    <div class="header-container">
        <div class="title">
            <a href="/"> <img src="images/logo_03.jpg" width="300" height=86> </a> 

            <div class="topnav-right">
                <a href="javascript:void(0);" class="icon" onclick="myFunction()"> <i class="fa fa-bars"></i> </a>
            </div>
            <div id="myLinks">
                <br>
            </div>
        </div> 
    </div>
</header>
    <br>
    <div class="page-content">
      <div class="wrapper">
        <div class="post">
  <br>
  <header class="post-header">
    <h1 class="display-4" align="center">Dynamic Crosswalk Scene Understanding for the Visually Impaired</h1>
    <p class="post-meta" align="center">By: Guoguang Hua</p>
  </header>

  <article class="post-content">
    <p align="center">
	<a href="https://www.tamu.edu/"><img src="images/logo_03.jpg" alt="Shenzhen University" height="86" width="300" /></a>
	<a href="https://www.insa-rennes.fr/"><img src="images/INSA.png" alt="INSA Rennes" height="225" width="300" /></a><br />
	<a href="https://www.arl.army.mil/"><img src="images/guangdong.jpg" alt="CCDC Army Research Laboratory" height="86" width="654" /></a></p>

<p align="center">
Shishun Tian, Minghuo Zheng, Wenbin Zou, Xia Li, and Lu Zhang<sup>*</sup><br />
1. <a href="http://ceie.szu.edu.cn/">Shenzhen University; </a>&emsp;2. <a href="https://www.insa-rennes.fr/">Guangdong KLIIP; </a> &emsp;3. <a href="http://iip.szu.edu.cn/">Guangdong KLIIP;<br />
<a href="https://unmannedlab.github.io/research/RELLIS-3D">[Website]</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9481094">[Paper]</a> <a href="https://github.com/SensingAI/WVD-SZU/">[Github]</a>
</p>

<h2 id="overview">Overview</h2>
<p>Independent mobility poses a great challenge to the visually impaired individuals. This paper proposes a novel system to understand dynamic crosswalk scenes,
which detects the key objects, such as crosswalk, vehicle, and pedestrian, and identifies pedestrian traffic light status. The indication of where and when to cross the road is provided to the visually impaired based on the crosswalk scene
understanding. Our proposed system is implemented on a head-mounted mobile device (SensingAI G1) equipped with an Intel RealSense camera and a cellphone, and provides
surrounding scene information to visually impaired individuals through audio signal. To validate the performance of the proposed system, we propose a crosswalk scene
understanding dataset which contains three sub-datasets: a pedestrian traffic light dataset with 7447 images, a dataset of key objects on the crossroad with 1006 images and a
crosswalk dataset with 3336 images. Extensive experiments demonstrated that the proposedsystem was robust and outperformed the state-of-the-art approaches. The experiment
conducted with the visually impaired subjects shows that the system is practical useful. </p>
<span>
	<p align="center"><img src="images/SensingaiG1.png" width="450" class="left" /></p> 
	<p align="center"><img src="images/crosswalk.png" width="450" class="right" /></p>
</span>
<h2 id="annotated-data">Annotated Data:</h2>
<h3 id="ontology">Category:</h3>
<p>With the goal of providing data to enhance unstructure environment navigation, we defined the WVD dataset. WVD contains diverse categories that can easily be spotted from a walk viewpoint, e.g., pedestrian-area and bike-lane. Visually impaired people who are trailed tend to struggle with walking straight, so they tend to touch the Blind to follow over a straightFor mobile robots, the definitions of safe regions are different, e.g., the sidewalk is higher than the bike-lane, and the bike-lane is higher than the road in safety factors. Overall, 20 categories are present in the data.</p>

<h3 id="images-statics">Images Statics:</h3>

<p align="center"><img src="file:///C|/Users/Administrator/Desktop/website/images/static2_00.jpg" width="855" class="center" /></p>

<h3 id="lidar-scans-statics">Note:</h3>
<p>(1) Two inset to better visualize some of categories.</p>
<p>(2)  If you can't access the file, please email every author with the title "WVD-SZU Aceess Request...".</p>

<h2 id="benchmarks">Benchmarks</h2>

<h3 id="image-semantic-segmenation">The MIoU Score of Models at Three Levels</h3>
<p align="center"><img src="images/Treelevel.jpg" width="485" class="center" /></p>

<h3 id="lidar-semantic-segmenation">Per Categories on The Testing Split</h3>
<p align="center"><img src="images/percategories.jpg" width="1000" class="center" /></p>
<p align="center"><img src="images/Results_00.jpg" width="885" class="center" /></p>


<h2 id="data-download">Data Download</h2>
	<p>Baidu Cloud Disk:</p>
		     <ul>
			<li>Link:<a href="https://pan.baidu.com/s/1KsVTrJcD4vilYRi7jVo7Xg">https://pan.baidu.com/s/1KsVTrJcD4vilYRi7jVo7Xg</a></li>
			<li>PassWord: akig </li>
		     </ul>
	<p>Google Cloud Disk:</p>
		     <ul>
			<li>Link:<a href="https://drive.google.com/drive/folders/1cMQwkv2UlqRPX-lnQT9dxmv9eiLgLViw?usp=drive_link">https://drive.google.com/drive/folders/1cMQwkv2UlqRPX-lnQT9dxmv9eiLgLViw?usp=drive_link</a></li>
		     </ul>
											  

									 
									 
<h2 id="citation">Citation</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@ARTICLE{9481094,
  author={Tian, Shishun and Zheng, Minghuo and Zou, Wenbin and Li, Xia and Zhang, Lu},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Dynamic Crosswalk Scene Understanding for the Visually Impaired}, 
  year={2021},
  volume={29},
  number={},
  pages={1478-1486},
  doi={10.1109/TNSRE.2021.3096379}}
</code></pre></div></div>

<h2 id="collaborator">Collaborator</h2>
<p><a href="https://www.arl.army.mil/"><img src="images/SensingAi.png" alt="Research Laboratory" width="358" height="182" class="center" /></a></p>

<h2 id="license">License</h2>
<p>All datasets and code on this page are copyright by us and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License.</p>

  </article>

	<div class="ds-thread" data-thread-key=/research/RELLIS-3D data-title=RELLIS-3D: A Multi-modal Dataset for Off-Road Robotics data-url=//research/RELLIS-3D></div>


</div>

      </div>
     
    </div>
  </body>
  <footer class="footer">
    <div class="footer-container">
        <div class="container">
            <div class="section left">
                &#169; 2023 SensingAI <br>
                CoLL. of Electronics and Information Engineering <br>
                Shenzhen University
            </div>
            <div class="section centerd">
                <br>
                <style>
    footer a:visited{
        color: #404040;
    }

    footer a:hover{
        color: #303030;
    }
</style>
            </div>
            <div class="section right">
                3123 TAMU<br>College Station, TX<br> 77845-3123
            </div>
        </div>
    </div>
</footer>

<script src="https://unmannedlab.github.io//js/jquery.slim.min.js"></script>
<script src="https://unmannedlab.github.io//js/bootstrap.min.js"></script>

</html>
